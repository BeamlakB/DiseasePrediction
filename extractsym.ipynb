{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "I used hugging face cli to access LLAMA2. In this code using LLAMA2,we extract the main symproms and ssave it in symptom-description.csv. \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bbekele1/.conda/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bbekele1/.conda/envs/llama/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\" #mistralai/Mistral-7B-v0.1 # meta-llama/Llama-2-7b-hf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",  # LLM task\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response(prompt: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a response from the Llama model.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The user's input/question for the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the model's response.\n",
    "    \"\"\"\n",
    "    sequences = llama_pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=256,\n",
    "    )\n",
    "    return (\"Chatbot:\", sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero shot \n",
    "prompt = ''' \n",
    "[INST]\n",
    "Extract main symptoms from the sentence above  ?  \"\n",
    "Text: My skin has been peeling, especially on my knees, elbows, and scalp. This peeling is often accompanied by a burning or stinging sensa\n",
    "'''\n",
    "\n",
    "zeroshot=get_llama_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero shot + Role\n",
    "prompt = ''' \n",
    "[INST]\n",
    "Extract main symptoms from the sentence above and retrun them only as tagged as a list called mainsymptoms? P \"\n",
    "Text:My skin has been peeling, especially on my knees, elbows, and scalp. This peeling is often accompanied by a burning or stinging sensa\n",
    "\n",
    "[/INST]\n",
    "'''\n",
    "zeroplusrole=get_llama_response(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Few-shot + Role\n",
    "prompt = ''' \n",
    "[INST]\n",
    "Extract main symptoms from the sentence above and retrun them only as tagged as a list called mainsymptoms? P \"\n",
    "Text: I have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. It is red, itchy, and covered in dry, scaly patches. \n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "[INST] <<SYS>>\n",
    "Extract main symptoms from the sentence above and retrun them only as tagged as a list called mainsymptoms?\n",
    "<</SYS>>\n",
    "Text: I have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. It is red, itchy, and covered in dry, scaly patches. \n",
    "[/INST]\n",
    "mainsymptoms=[\"skin_rash\", \"red_skin\", \"itchy\", \"dry\", \"scaly_patches\"]\n",
    "[INST]\n",
    "My skin has been peeling, especially on my knees, elbows, and scalp. This peeling is often accompanied by a burning or stinging sensa\n",
    "[/INST]\n",
    "'''\n",
    "fewshot=get_llama_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero shot output--> Note: These are the main symptoms mentioned in the sentence, but there may be other symptoms present depending on the context of the text.\n",
      "zero plus role output--> mainsymptoms = [\"peeling\", \"burning\", \"stinging sensation\"]\n",
      "fews shot plus role --> mainsymptoms=[\"peeling_skin\", \"burning_sensation\", \"stinging_sensation\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"zero shot output-->\", (zeroshot[1].split('\\n')[-1]))\n",
    "print (\"zero plus role output-->\", zeroplusrole[1].split('\\n')[-1])\n",
    "\n",
    "#Few shot had a better out put and was structured the way we wanted it so we are usign \n",
    "#fewshot prompt to annotate the data \n",
    "print (\"fews shot plus role -->\", fewshot[1].split('\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt can called usign this function \n",
    "def create_prompt(inp):\n",
    "    return f'''\n",
    "    [INST] <<SYS>>\n",
    "    Extract main symptoms one or two words from the text, return just the symptom tagged as a list called mainsymptoms?\n",
    "    <</SYS>>\n",
    "    Text: My nails have small dents or pits in them, and they often feel inflammatory and tender to the touch. Even there are minor rashes on my arms.\n",
    "    [/INST]\n",
    "    mainsymptoms=[dents, inflammation, skin_rash, tender_to_touch]\n",
    "    [INST]\n",
    "    {inp}\n",
    "    [/INST]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extracting symptoms in bulk </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function cleans the output and prints just the extracted symptom \n",
    "def return_list (my_output):\n",
    "    splitted=my_output.split (\"\\n\")\n",
    "    #mainsym=(splitted[10].split(\"=\")[1])\n",
    "    try:\n",
    "        mainsym = (splitted[10].split(\"=\")[1])\n",
    "    except IndexError:\n",
    "        try:\n",
    "            mainsym = (splitted[11].split(\"=\")[1])\n",
    "        except IndexError:\n",
    "            mainsym = \"NaN\"  # Handle the case where both indices are out of range\n",
    "            return (mainsym)\n",
    "    \n",
    "    mainsym= mainsym.split(\",\")\n",
    "    #print  (mainsym.split (\",\"))\n",
    "    sym=[]\n",
    "    for i in mainsym:\n",
    "        k=i.replace(\"\\\"\", \"\")\n",
    "        k= k.replace (\"[\",\"\")\n",
    "        k= k.replace (\"]\",\"\")\n",
    "        sym.append(k)\n",
    "    return (sym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read symptom-description , col extracted_sym will be filled  \n",
    "import pandas as pd\n",
    "df= pd.read_csv(\"symptom-description.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it in parrallel of 32 batches for efficent use of the gpu\n",
    "batch_size = 32\n",
    "inputs = df.iloc[:, 1].tolist()\n",
    "outputs = []\n",
    "\n",
    "# Processing in batches\n",
    "for i in range(0, len(inputs), batch_size):\n",
    "    batch_inputs = inputs[i:i+batch_size]\n",
    "    prompts = [create_prompt(inp) for inp in batch_inputs]\n",
    "    batch_outputs = llama_pipeline(prompts)  # Adjust max_length as needed\n",
    "    \n",
    "    for output in batch_outputs:\n",
    "        mylist = return_list(output[0]['generated_text'])\n",
    "        mylist=(\",\".join(mylist))\n",
    "        outputs.append(mylist)\n",
    "\n",
    "# Convert the outputs to strings before assigning to the DataFrame\n",
    "outputs_str = [output for output in outputs]\n",
    "\n",
    "# Adding the results back to the dataframe\n",
    "df.iloc[:len(outputs_str), 2] = outputs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the anotated symptom description \n",
    "df.to_csv(\"symptom-description.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
